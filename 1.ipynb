{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33cd490a",
   "metadata": {},
   "source": [
    "## openrouter  api call for large context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29509e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "token = os.environ.get(\"OPENROUTER_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d6ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='**Owl‚ÄëContext Notes ‚Äì Reasoning‚ÄëBased Large Language Models**  \\n*(A hierarchical ‚Äúflight‚Äëmap‚Äù for the brain, written in the style of an owl‚Äôs layered wisdom)*  \\n\\n---\\n\\n## 1Ô∏è‚É£  The Owl‚Äôs Perch ‚Äì Lecture Overview  \\n\\n| **Perch Level** | **What the Owl Holds** | **Why It Matters** |\\n|-----------------|------------------------|--------------------|\\n| **Top Branch** | *Why reasoning matters* ‚Äì LLMs that only ‚Äútalk like humans‚Äù (e.g., GPT‚Äë3.5) lack the ability to *think* step‚Äëby‚Äëstep. | Sets the problem: we need models that *reason*, not just *regurgitate*. |\\n| **Middle Limb** | *Two‚Äëstep evolution* ‚Äì From ‚Äúanswer‚Äëonly‚Äù to **reasoning LLMs** (e.g., OpenAI‚Äôs O‚ÇÇ, DeepMind‚Äôs COG, etc.). | Introduces the timeline (Dec\\u202f2022\\u202f‚Üí\\u202fnow) when reasoning became a research priority. |\\n| **Bottom Leaf** | *Goal of the series* ‚Äì Understand *how* we can endow LLMs with reasoning powers. | Guides the rest of the course (methods, experiments, hands‚Äëon). |\\n\\n---\\n\\n## 2Ô∏è‚É£  The Branch of **Inference‚ÄëTime Compute Scaling**  \\n\\n> **Analogy:** A chess grandmaster spends minutes weighing variations before moving; a model that ‚Äúthinks longer‚Äù uses more compute at inference time and often gets a better answer.  \\n\\n### 2.1  Core Idea  \\n- **Definition:** *Test‚Äëtime (inference‚Äëtime) compute* = the amount of computational resources (tokens, forward passes, FLOPs) the model spends **before** emitting its final output.  \\n- **Intuition:** When we tell a model to ‚Äúthink more,‚Äù we increase the depth/width of its internal chain of reasoning, which typically raises accuracy on complex tasks.\\n\\n### 2.2  Example Walk‚ÄëThrough  \\n\\n| Step | Action | Tokens Used | Reasoning |\\n|------|--------|------------|-----------|\\n| 1 | ‚ÄúRoger started with 5 balls.‚Äù | 4 | Sets the initial state. |\\n| 2 | ‚ÄúTwo cans √ó 3 balls each = 6 balls.‚Äù | 11 | Performs multiplication. |\\n| 3 | ‚Äú5 + 6 = 11.‚Äù | 5 | Final addition. |\\n| **Total** | **Answer = 11** | **‚âà20 tokens** (vs. 3 tokens for a direct answer) | More tokens ‚Üí more deliberation ‚Üí same or improved correctness. |\\n\\n### 2.3  Empirical Scaling Plot (PaLM example)  \\n\\n- **X‚Äëaxis:** Model parameter count (62\\u202fB ‚Üí 540\\u202fB).  \\n- **Y‚Äëaxis:** Accuracy on GSM‚Äë8K (arithmetic reasoning).  \\n- **Observations:**  \\n  1. **Standard prompting** ‚Üí low accuracy (~5‚Äì10\\u202f%).  \\n  2. **Chain‚Äëof‚ÄëThought prompting** ‚Üí steep rise; larger models achieve near‚Äësupervised‚Äëfine‚Äëtune performance *without* any weight updates.  \\n  3. **Zero‚ÄëShot CoT** (just ‚ÄúLet‚Äôs think step‚Äëby‚Äëstep‚Äù) ‚Üí similar gains, especially on 540\\u202fB models.\\n\\n### 2.4  Why More Compute Helps  \\n\\n| Mechanism | Explanation |\\n|-----------|-------------|\\n| **Deeper reasoning pathways** | Extra forward passes let the model explore intermediate states, akin to ‚Äúmental simulation.‚Äù |\\n| **Error correction** | Mistakes made early can be revised when the model continues reasoning, fixing semantic slips (e.g., confusing ‚Äú4\\u202f√ó\\u202f6 vs. 48√∑6‚Äù). |\\n| **Emergent ability** | Scaling reveals a *threshold* where the model ‚Äúgets‚Äù the prompting nudge and can follow reasoning chains reliably. |\\n\\n---\\n\\n## 3Ô∏è‚É£  The **Chain‚Äëof‚ÄëThought (CoT)** Branch  \\n\\n### 3.1  What It Is  \\n- **Definition:** Adding a **series of intermediate steps** (natural‚Äëlanguage ‚Äúrationals‚Äù) to the output of a prompt.  \\n- **Implementation:** Provide *few‚Äëshot* examples where **input ‚Üí reasoning steps ‚Üí final answer** are shown.  \\n\\n### 3.2  Structural Anatomy  \\n\\n```\\nInput:  Roger has 5 tennis balls. He buys 2 cans of 3 balls each.\\nOutput (CoT):\\n  1. Roger started with 5 balls.          ‚Üê 4 tokens\\n  2. Two cans √ó 3 balls = 6 balls.       ‚Üê 11 tokens\\n  3. 5 + 6 = 11.                         ‚Üê 5 tokens\\n  4. Answer: 11.                         ‚Üê final token\\n```\\n\\n*Key nuance:* The model **generates the reasoning** itself once the pattern is demonstrated.\\n\\n### 3.3  Why It Works  \\n\\n| Insight | Detail |\\n|---------|--------|\\n| **Rationales bridge gaps** | Natural language ‚Äúrationals‚Äù provide a *semantic scaffold* that aligns model activations with logical steps. |\\n| **No architectural change** | Only the prompt changes; the underlying model stays identical. |\\n| **Few‚Äëshot prompting** | 10‚Äë20 demonstration pairs are enough for the model to internalize the reasoning pattern. |\\n\\n---\\n\\n## 4Ô∏è‚É£  **Few‚ÄëShot vs. Zero‚ÄëShot CoT**  \\n\\n| Feature | Few‚ÄëShot CoT | Zero‚ÄëShot CoT |\\n|---------|--------------|---------------|\\n| **Prompt content** | Input‚ÄëOutput pairs **with** explicit reasoning steps. | No examples; only a cue like ‚ÄúLet‚Äôs think step‚Äëby‚Äëstep.‚Äù |\\n| **Cognitive load on model** | Must match the *format* of the examples (structure, token count). | Must *self‚Äëgenerate* a reasoning chain from the cue alone. |\\n| **Typical performance** | Often higher on very large models because they can copy the demonstrated style. | Slightly lower but still improves over vanilla prompting; easier to deploy. |\\n| **Implementation cost** | Requires curating demonstration set. | Minimal overhead ‚Äì just a short instruction. |\\n\\n---\\n\\n## 5Ô∏è‚É£  Comparative Experiments (Hands‚ÄëOn Lens)\\n\\n### 5.1  Project\\u202f1 ‚Äì *Model‚ÄëSize √ó CoT Effect*  \\n- **Goal:** Empirically verify that CoT only shines on sufficiently large models.  \\n- **Setup:**  \\n  - Datasets: GSM‚Äë8K (arithmetic), logical/symbolic reasoning sets.  \\n  - Models: FLAN‚ÄëT5‚Äësmall (80\\u202fM), FLAN‚ÄëT5‚Äëbase, FLAN‚ÄëT5‚Äëlarge, LLaMA‚Äë52\\u202fB, LLaMA‚Äë7\\u202fB, TinyLlama‚Äë1.1\\u202fB.  \\n  - Evaluation function: `is_correct(answer, ground_truth)`.  \\n- **Findings (in lecture):**  \\n  - Accuracy ‚âà 0‚Äì5\\u202f% for all models ‚â§ 7\\u202fB with CoT.  \\n  - Accuracy climbs sharply only beyond ~50\\u202fB parameters.  \\n  - Matches paper‚Äôs claim: *Chain‚Äëof‚ÄëThought is an emergent ability of model scaling*.  \\n\\n### 5.2  Project\\u202f2 ‚Äì *Three Prompting Strategies*  \\n- **Baselines:**  \\n  1. **Vanilla Few‚ÄëShot** (direct answer).  \\n  2. **Few‚ÄëShot CoT** (explicit steps).  \\n  3. **Zero‚ÄëShot CoT** (‚ÄúLet‚Äôs think step‚Äëby‚Äëstep‚Äù).  \\n- **Models Tested:** FLAN‚Äë52\\u202fB & LLaMA‚Äë7\\u202fB.  \\n- **Tasks:** Logical puzzles, symbolic flips, simple commonsense questions.  \\n- **Result Summary:**  \\n  - Zero‚ÄëShot CoT outperforms both baselines (‚âà20\\u202f% ‚Üí 0\\u202f% vanilla).  \\n  - Few‚ÄëShot CoT remains superior when enough demonstrations are provided.  \\n  - Demonstrates that *a single cue* (‚Äúthink step‚Äëby‚Äëstep‚Äù) can unlock reasoning without curated examples.  \\n\\n---\\n\\n## 6Ô∏è‚É£  **Key Takeaways ‚Äì The Owl‚Äôs Nuggets**  \\n\\n| **Takeaway** | **Explanation (Owl‚ÄëStyle)** |\\n|--------------|-----------------------------|\\n| **Reasoning is a scaling phenomenon** | Like a fledgling eventually learning to soar, only the biggest brains can reliably follow multi‚Äëstep reasoning when prompted. |\\n| **Inference‚Äëtime compute ‚â† training cost** | You don‚Äôt need to retrain; you simply spend more ‚Äúthinking time‚Äù (tokens) at inference, which mobilizes latent reasoning pathways. |\\n| **Chain‚Äëof‚ÄëThought is prompt engineering, not architecture** | The model‚Äôs *knowledge* stays the same; the *behaviour* emerges from how you frame the question. |\\n| **Zero‚ÄëShot CoT is a low‚Äëfriction entry point** | Just add ‚ÄúLet‚Äôs think step‚Äëby‚Äëstep‚Äù ‚Äì cheap, universal, works even without demonstration examples. |\\n| **Empirical gains are robust across domains** | Arithmetic, common‚Äësense, and symbolic tasks all benefit, though the magnitude varies (bigger for logical puzzles, modest for simple commonsense). |\\n| **Model size matters, but prompt design matters more** | A modest cue can unlock massive performance jumps if the model is large enough; conversely, even a large model may flounder with a poorly phrased prompt. |\\n| **Practical implication** | For production, you can often avoid fine‚Äëtuning: just switch to CoT/Zero‚ÄëShot CoT prompts and allocate a modest compute budget at inference. |\\n\\n---\\n\\n## 7Ô∏è‚É£  **Suggested Reading Path (Owl‚ÄëGuided Bibliography)**  \\n\\n1. **\"Chain of Thought Prompting Elicits Reasoning in Large Language Models\"** ‚Äì Google Brain, 2022.  \\n2. **\"Solving NLP Tasks with Zero‚ÄëShot CoT\"** ‚Äì Wei et\\u202fal., 2022 (arXiv).  \\n3. **\"Scaling Laws for Neural Language Models\"** ‚Äì Kaplan et\\u202fal., 2020 (parameter‚Äëvs‚Äëperformance trends).  \\n4. **\"Emergent Abilities of Large Language Models\"** ‚Äì Wei, Lee, et\\u202fal., 2022.  \\n5. **Hands‚ÄëOn Colab notebooks** ‚Äì shared during lecture (install `transformers`, `datasets`, `accelerate`; load GSM‚Äë8K; test FLAN‚ÄëT5‚Äësmall ‚Üí LLaMA‚Äë7B).  \\n\\n---\\n\\n## 8Ô∏è‚É£  **Next Flight Steps (Course Continuation)**  \\n\\n- **Module\\u202f1 Recap:** Inference‚Äëtime compute scaling ‚Üí deeper dive into *budget‚Äëaware* reasoning (e.g., dynamic token allocation, ‚Äúself‚Äëconsistency‚Äù).  \\n- **Module\\u202f2 Preview:** *Model‚Äëbased* reasoning (e.g., Retrieval‚ÄëAugmented Generation, Tool‚ÄëUse, Tool‚Äëformer).  \\n- **Hands‚ÄëOn Labs:**  \\n  1. **Lab\\u202fA:** Build your own CoT few‚Äëshot prompts for GSM‚Äë8K; evaluate scaling curves.  \\n  2. **Lab\\u202fB:** Implement Zero‚ÄëShot CoT; compare with vanilla prompting across logical tasks.  \\n  3. **Lab\\u202fC:** Run a ‚Äúfrom‚Äëscratch‚Äù 7\\u202fB LLaMA clone; test CoT on commonsense QA.  \\n\\n---\\n\\n### üéØ  Bottom‚ÄëLine (Owl‚ÄëWisdom)\\n\\n> **When we give LLMs *time* to think‚Äîby allocating extra inference compute and nudging them with a simple ‚Äúthink step‚Äëby‚Äëstep‚Äù cue‚Äîwe unlock a latent reasoning engine that scales with model size.**  \\n> \\n> *Use the right prompt, give the model enough ‚Äúflight time,‚Äù and watch it transform from a chatterbox into a problem‚Äësolving predator.*  \\n\\n--- \\n\\n*End of Owl‚ÄëContext notes.*  \\n\\n\\n\\n--- \\n\\n*Prepared for students seeking a deep, structured understanding of reasoning‚Äëbased LLMs, with an emphasis on inference‚Äëtime compute scaling, Chain‚Äëof‚ÄëThought prompting, and practical experimentation.*', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning='We need to make notes: a very comprehensive note from lecture and discussion, under key points with extensive details key takeaways, owl context (maybe outline). The user wants \"please make notes from lecture and discussion.Owl context the notes with key points extensively described key takeaways owl context\". So likely they want structured notes with headings, bullet points, etc. Provide comprehensive notes, covering overview, inference time compute scaling, chain of thought, few-shot, zero-shot, example tasks, empirical results, observations, projects, etc. Use \"Owl context\" maybe means present an outline like an owl diagram? Not sure. Might interpret as \"in owl context\" meaning like an owl\\'s perspective: hierarchical notes. Could structure as nested bullet points representing layers.\\n\\nI\\'ll produce notes with sections: Lecture Overview, Key Concepts, Inference Time Compute Scaling, Chain of Thought Prompting, Few-Shot COT, Zero-Shot COT, Empirical Findings, Model Scaling Effects, Hands-On Projects, Takeaways, References. Use bullet points with detailed elaboration. Also include owl context maybe as metaphor: central idea as the owl, branches as thoughts. I\\'ll incorporate an \"Owl Context\" heading explaining hierarchical view.\\n\\nLet\\'s craft.\\n', reasoning_details=[{'format': 'unknown', 'index': 0, 'type': 'reasoning.text', 'text': 'We need to make notes: a very comprehensive note from lecture and discussion, under key points with extensive details key takeaways, owl context (maybe outline). The user wants \"please make notes from lecture and discussion.Owl context the notes with key points extensively described key takeaways owl context\". So likely they want structured notes with headings, bullet points, etc. Provide comprehensive notes, covering overview, inference time compute scaling, chain of thought, few-shot, zero-shot, example tasks, empirical results, observations, projects, etc. Use \"Owl context\" maybe means present an outline like an owl diagram? Not sure. Might interpret as \"in owl context\" meaning like an owl\\'s perspective: hierarchical notes. Could structure as nested bullet points representing layers.\\n\\nI\\'ll produce notes with sections: Lecture Overview, Key Concepts, Inference Time Compute Scaling, Chain of Thought Prompting, Few-Shot COT, Zero-Shot COT, Empirical Findings, Model Scaling Effects, Hands-On Projects, Takeaways, References. Use bullet points with detailed elaboration. Also include owl context maybe as metaphor: central idea as the owl, branches as thoughts. I\\'ll incorporate an \"Owl Context\" heading explaining hierarchical view.\\n\\nLet\\'s craft.\\n'}])\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=token,\n",
    ")\n",
    "\n",
    "with open(\"text.txt\",'r') as f:\n",
    "    content=f.read()\n",
    "\n",
    "# First API call with reasoning\n",
    "response = client.chat.completions.create(\n",
    "  model=\"nvidia/nemotron-3-nano-30b-a3b:free\",\n",
    "  messages=[\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "          }\n",
    "        ],\n",
    "  extra_body={\"reasoning\": {\"enabled\": True}}\n",
    ")\n",
    "\n",
    "# Extract the assistant message with reasoning_details\n",
    "response = response.choices[0].message\n",
    "print(response)\n",
    "\n",
    "# Preserve the assistant message with reasoning_details\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"How many r's are in the word 'strawberry'?\"},\n",
    "  {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": response.content,\n",
    "    \"reasoning_details\": response.reasoning_details  # Pass back unmodified\n",
    "  },\n",
    "  {\"role\": \"user\", \"content\": \"Are you sure? Think carefully.\"}\n",
    "]\n",
    "\n",
    "# Second API call - model continues reasoning from where it left off\n",
    "response2 = client.chat.completions.create(\n",
    "  model=\"nvidia/nemotron-3-nano-30b-a3b:free\",\n",
    "  messages=messages,\n",
    "  extra_body={\"reasoning\": {\"enabled\": True}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14293bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='**Owl‚ÄëContext Notes ‚Äì Reasoning‚ÄëBased Large Language Models**  \\n*(A hierarchical ‚Äúflight‚Äëmap‚Äù for the brain, written in the style of an owl‚Äôs layered wisdom)*  \\n\\n---\\n\\n## 1Ô∏è‚É£  The Owl‚Äôs Perch ‚Äì Lecture Overview  \\n\\n| **Perch Level** | **What the Owl Holds** | **Why It Matters** |\\n|-----------------|------------------------|--------------------|\\n| **Top Branch** | *Why reasoning matters* ‚Äì LLMs that only ‚Äútalk like humans‚Äù (e.g., GPT‚Äë3.5) lack the ability to *think* step‚Äëby‚Äëstep. | Sets the problem: we need models that *reason*, not just *regurgitate*. |\\n| **Middle Limb** | *Two‚Äëstep evolution* ‚Äì From ‚Äúanswer‚Äëonly‚Äù to **reasoning LLMs** (e.g., OpenAI‚Äôs O‚ÇÇ, DeepMind‚Äôs COG, etc.). | Introduces the timeline (Dec\\u202f2022\\u202f‚Üí\\u202fnow) when reasoning became a research priority. |\\n| **Bottom Leaf** | *Goal of the series* ‚Äì Understand *how* we can endow LLMs with reasoning powers. | Guides the rest of the course (methods, experiments, hands‚Äëon). |\\n\\n---\\n\\n## 2Ô∏è‚É£  The Branch of **Inference‚ÄëTime Compute Scaling**  \\n\\n> **Analogy:** A chess grandmaster spends minutes weighing variations before moving; a model that ‚Äúthinks longer‚Äù uses more compute at inference time and often gets a better answer.  \\n\\n### 2.1  Core Idea  \\n- **Definition:** *Test‚Äëtime (inference‚Äëtime) compute* = the amount of computational resources (tokens, forward passes, FLOPs) the model spends **before** emitting its final output.  \\n- **Intuition:** When we tell a model to ‚Äúthink more,‚Äù we increase the depth/width of its internal chain of reasoning, which typically raises accuracy on complex tasks.\\n\\n### 2.2  Example Walk‚ÄëThrough  \\n\\n| Step | Action | Tokens Used | Reasoning |\\n|------|--------|------------|-----------|\\n| 1 | ‚ÄúRoger started with 5 balls.‚Äù | 4 | Sets the initial state. |\\n| 2 | ‚ÄúTwo cans √ó 3 balls each = 6 balls.‚Äù | 11 | Performs multiplication. |\\n| 3 | ‚Äú5 + 6 = 11.‚Äù | 5 | Final addition. |\\n| **Total** | **Answer = 11** | **‚âà20 tokens** (vs. 3 tokens for a direct answer) | More tokens ‚Üí more deliberation ‚Üí same or improved correctness. |\\n\\n### 2.3  Empirical Scaling Plot (PaLM example)  \\n\\n- **X‚Äëaxis:** Model parameter count (62\\u202fB ‚Üí 540\\u202fB).  \\n- **Y‚Äëaxis:** Accuracy on GSM‚Äë8K (arithmetic reasoning).  \\n- **Observations:**  \\n  1. **Standard prompting** ‚Üí low accuracy (~5‚Äì10\\u202f%).  \\n  2. **Chain‚Äëof‚ÄëThought prompting** ‚Üí steep rise; larger models achieve near‚Äësupervised‚Äëfine‚Äëtune performance *without* any weight updates.  \\n  3. **Zero‚ÄëShot CoT** (just ‚ÄúLet‚Äôs think step‚Äëby‚Äëstep‚Äù) ‚Üí similar gains, especially on 540\\u202fB models.\\n\\n### 2.4  Why More Compute Helps  \\n\\n| Mechanism | Explanation |\\n|-----------|-------------|\\n| **Deeper reasoning pathways** | Extra forward passes let the model explore intermediate states, akin to ‚Äúmental simulation.‚Äù |\\n| **Error correction** | Mistakes made early can be revised when the model continues reasoning, fixing semantic slips (e.g., confusing ‚Äú4\\u202f√ó\\u202f6 vs. 48√∑6‚Äù). |\\n| **Emergent ability** | Scaling reveals a *threshold* where the model ‚Äúgets‚Äù the prompting nudge and can follow reasoning chains reliably. |\\n\\n---\\n\\n## 3Ô∏è‚É£  The **Chain‚Äëof‚ÄëThought (CoT)** Branch  \\n\\n### 3.1  What It Is  \\n- **Definition:** Adding a **series of intermediate steps** (natural‚Äëlanguage ‚Äúrationals‚Äù) to the output of a prompt.  \\n- **Implementation:** Provide *few‚Äëshot* examples where **input ‚Üí reasoning steps ‚Üí final answer** are shown.  \\n\\n### 3.2  Structural Anatomy  \\n\\n```\\nInput:  Roger has 5 tennis balls. He buys 2 cans of 3 balls each.\\nOutput (CoT):\\n  1. Roger started with 5 balls.          ‚Üê 4 tokens\\n  2. Two cans √ó 3 balls = 6 balls.       ‚Üê 11 tokens\\n  3. 5 + 6 = 11.                         ‚Üê 5 tokens\\n  4. Answer: 11.                         ‚Üê final token\\n```\\n\\n*Key nuance:* The model **generates the reasoning** itself once the pattern is demonstrated.\\n\\n### 3.3  Why It Works  \\n\\n| Insight | Detail |\\n|---------|--------|\\n| **Rationales bridge gaps** | Natural language ‚Äúrationals‚Äù provide a *semantic scaffold* that aligns model activations with logical steps. |\\n| **No architectural change** | Only the prompt changes; the underlying model stays identical. |\\n| **Few‚Äëshot prompting** | 10‚Äë20 demonstration pairs are enough for the model to internalize the reasoning pattern. |\\n\\n---\\n\\n## 4Ô∏è‚É£  **Few‚ÄëShot vs. Zero‚ÄëShot CoT**  \\n\\n| Feature | Few‚ÄëShot CoT | Zero‚ÄëShot CoT |\\n|---------|--------------|---------------|\\n| **Prompt content** | Input‚ÄëOutput pairs **with** explicit reasoning steps. | No examples; only a cue like ‚ÄúLet‚Äôs think step‚Äëby‚Äëstep.‚Äù |\\n| **Cognitive load on model** | Must match the *format* of the examples (structure, token count). | Must *self‚Äëgenerate* a reasoning chain from the cue alone. |\\n| **Typical performance** | Often higher on very large models because they can copy the demonstrated style. | Slightly lower but still improves over vanilla prompting; easier to deploy. |\\n| **Implementation cost** | Requires curating demonstration set. | Minimal overhead ‚Äì just a short instruction. |\\n\\n---\\n\\n## 5Ô∏è‚É£  Comparative Experiments (Hands‚ÄëOn Lens)\\n\\n### 5.1  Project\\u202f1 ‚Äì *Model‚ÄëSize √ó CoT Effect*  \\n- **Goal:** Empirically verify that CoT only shines on sufficiently large models.  \\n- **Setup:**  \\n  - Datasets: GSM‚Äë8K (arithmetic), logical/symbolic reasoning sets.  \\n  - Models: FLAN‚ÄëT5‚Äësmall (80\\u202fM), FLAN‚ÄëT5‚Äëbase, FLAN‚ÄëT5‚Äëlarge, LLaMA‚Äë52\\u202fB, LLaMA‚Äë7\\u202fB, TinyLlama‚Äë1.1\\u202fB.  \\n  - Evaluation function: `is_correct(answer, ground_truth)`.  \\n- **Findings (in lecture):**  \\n  - Accuracy ‚âà 0‚Äì5\\u202f% for all models ‚â§ 7\\u202fB with CoT.  \\n  - Accuracy climbs sharply only beyond ~50\\u202fB parameters.  \\n  - Matches paper‚Äôs claim: *Chain‚Äëof‚ÄëThought is an emergent ability of model scaling*.  \\n\\n### 5.2  Project\\u202f2 ‚Äì *Three Prompting Strategies*  \\n- **Baselines:**  \\n  1. **Vanilla Few‚ÄëShot** (direct answer).  \\n  2. **Few‚ÄëShot CoT** (explicit steps).  \\n  3. **Zero‚ÄëShot CoT** (‚ÄúLet‚Äôs think step‚Äëby‚Äëstep‚Äù).  \\n- **Models Tested:** FLAN‚Äë52\\u202fB & LLaMA‚Äë7\\u202fB.  \\n- **Tasks:** Logical puzzles, symbolic flips, simple commonsense questions.  \\n- **Result Summary:**  \\n  - Zero‚ÄëShot CoT outperforms both baselines (‚âà20\\u202f% ‚Üí 0\\u202f% vanilla).  \\n  - Few‚ÄëShot CoT remains superior when enough demonstrations are provided.  \\n  - Demonstrates that *a single cue* (‚Äúthink step‚Äëby‚Äëstep‚Äù) can unlock reasoning without curated examples.  \\n\\n---\\n\\n## 6Ô∏è‚É£  **Key Takeaways ‚Äì The Owl‚Äôs Nuggets**  \\n\\n| **Takeaway** | **Explanation (Owl‚ÄëStyle)** |\\n|--------------|-----------------------------|\\n| **Reasoning is a scaling phenomenon** | Like a fledgling eventually learning to soar, only the biggest brains can reliably follow multi‚Äëstep reasoning when prompted. |\\n| **Inference‚Äëtime compute ‚â† training cost** | You don‚Äôt need to retrain; you simply spend more ‚Äúthinking time‚Äù (tokens) at inference, which mobilizes latent reasoning pathways. |\\n| **Chain‚Äëof‚ÄëThought is prompt engineering, not architecture** | The model‚Äôs *knowledge* stays the same; the *behaviour* emerges from how you frame the question. |\\n| **Zero‚ÄëShot CoT is a low‚Äëfriction entry point** | Just add ‚ÄúLet‚Äôs think step‚Äëby‚Äëstep‚Äù ‚Äì cheap, universal, works even without demonstration examples. |\\n| **Empirical gains are robust across domains** | Arithmetic, common‚Äësense, and symbolic tasks all benefit, though the magnitude varies (bigger for logical puzzles, modest for simple commonsense). |\\n| **Model size matters, but prompt design matters more** | A modest cue can unlock massive performance jumps if the model is large enough; conversely, even a large model may flounder with a poorly phrased prompt. |\\n| **Practical implication** | For production, you can often avoid fine‚Äëtuning: just switch to CoT/Zero‚ÄëShot CoT prompts and allocate a modest compute budget at inference. |\\n\\n---\\n\\n## 7Ô∏è‚É£  **Suggested Reading Path (Owl‚ÄëGuided Bibliography)**  \\n\\n1. **\"Chain of Thought Prompting Elicits Reasoning in Large Language Models\"** ‚Äì Google Brain, 2022.  \\n2. **\"Solving NLP Tasks with Zero‚ÄëShot CoT\"** ‚Äì Wei et\\u202fal., 2022 (arXiv).  \\n3. **\"Scaling Laws for Neural Language Models\"** ‚Äì Kaplan et\\u202fal., 2020 (parameter‚Äëvs‚Äëperformance trends).  \\n4. **\"Emergent Abilities of Large Language Models\"** ‚Äì Wei, Lee, et\\u202fal., 2022.  \\n5. **Hands‚ÄëOn Colab notebooks** ‚Äì shared during lecture (install `transformers`, `datasets`, `accelerate`; load GSM‚Äë8K; test FLAN‚ÄëT5‚Äësmall ‚Üí LLaMA‚Äë7B).  \\n\\n---\\n\\n## 8Ô∏è‚É£  **Next Flight Steps (Course Continuation)**  \\n\\n- **Module\\u202f1 Recap:** Inference‚Äëtime compute scaling ‚Üí deeper dive into *budget‚Äëaware* reasoning (e.g., dynamic token allocation, ‚Äúself‚Äëconsistency‚Äù).  \\n- **Module\\u202f2 Preview:** *Model‚Äëbased* reasoning (e.g., Retrieval‚ÄëAugmented Generation, Tool‚ÄëUse, Tool‚Äëformer).  \\n- **Hands‚ÄëOn Labs:**  \\n  1. **Lab\\u202fA:** Build your own CoT few‚Äëshot prompts for GSM‚Äë8K; evaluate scaling curves.  \\n  2. **Lab\\u202fB:** Implement Zero‚ÄëShot CoT; compare with vanilla prompting across logical tasks.  \\n  3. **Lab\\u202fC:** Run a ‚Äúfrom‚Äëscratch‚Äù 7\\u202fB LLaMA clone; test CoT on commonsense QA.  \\n\\n---\\n\\n### üéØ  Bottom‚ÄëLine (Owl‚ÄëWisdom)\\n\\n> **When we give LLMs *time* to think‚Äîby allocating extra inference compute and nudging them with a simple ‚Äúthink step‚Äëby‚Äëstep‚Äù cue‚Äîwe unlock a latent reasoning engine that scales with model size.**  \\n> \\n> *Use the right prompt, give the model enough ‚Äúflight time,‚Äù and watch it transform from a chatterbox into a problem‚Äësolving predator.*  \\n\\n--- \\n\\n*End of Owl‚ÄëContext notes.*  \\n\\n\\n\\n--- \\n\\n*Prepared for students seeking a deep, structured understanding of reasoning‚Äëbased LLMs, with an emphasis on inference‚Äëtime compute scaling, Chain‚Äëof‚ÄëThought prompting, and practical experimentation.*', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning='We need to make notes: a very comprehensive note from lecture and discussion, under key points with extensive details key takeaways, owl context (maybe outline). The user wants \"please make notes from lecture and discussion.Owl context the notes with key points extensively described key takeaways owl context\". So likely they want structured notes with headings, bullet points, etc. Provide comprehensive notes, covering overview, inference time compute scaling, chain of thought, few-shot, zero-shot, example tasks, empirical results, observations, projects, etc. Use \"Owl context\" maybe means present an outline like an owl diagram? Not sure. Might interpret as \"in owl context\" meaning like an owl\\'s perspective: hierarchical notes. Could structure as nested bullet points representing layers.\\n\\nI\\'ll produce notes with sections: Lecture Overview, Key Concepts, Inference Time Compute Scaling, Chain of Thought Prompting, Few-Shot COT, Zero-Shot COT, Empirical Findings, Model Scaling Effects, Hands-On Projects, Takeaways, References. Use bullet points with detailed elaboration. Also include owl context maybe as metaphor: central idea as the owl, branches as thoughts. I\\'ll incorporate an \"Owl Context\" heading explaining hierarchical view.\\n\\nLet\\'s craft.\\n', reasoning_details=[{'format': 'unknown', 'index': 0, 'type': 'reasoning.text', 'text': 'We need to make notes: a very comprehensive note from lecture and discussion, under key points with extensive details key takeaways, owl context (maybe outline). The user wants \"please make notes from lecture and discussion.Owl context the notes with key points extensively described key takeaways owl context\". So likely they want structured notes with headings, bullet points, etc. Provide comprehensive notes, covering overview, inference time compute scaling, chain of thought, few-shot, zero-shot, example tasks, empirical results, observations, projects, etc. Use \"Owl context\" maybe means present an outline like an owl diagram? Not sure. Might interpret as \"in owl context\" meaning like an owl\\'s perspective: hierarchical notes. Could structure as nested bullet points representing layers.\\n\\nI\\'ll produce notes with sections: Lecture Overview, Key Concepts, Inference Time Compute Scaling, Chain of Thought Prompting, Few-Shot COT, Zero-Shot COT, Empirical Findings, Model Scaling Effects, Hands-On Projects, Takeaways, References. Use bullet points with detailed elaboration. Also include owl context maybe as metaphor: central idea as the owl, branches as thoughts. I\\'ll incorporate an \"Owl Context\" heading explaining hierarchical view.\\n\\nLet\\'s craft.\\n'}])\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d39907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      " **Owl‚ÄëContext Notes ‚Äì Reasoning‚ÄëBased Large Language Models**  \n",
      "*(A hierarchical ‚Äúflight‚Äëmap‚Äù for the brain, written in the style of an owl‚Äôs layered wisdom)*  \n",
      "\n",
      "---\n",
      "\n",
      "## 1Ô∏è‚É£  The Owl‚Äôs Perch ‚Äì Lecture Overview  \n",
      "\n",
      "| **Perch Level** | **What the Owl Holds** | **Why It Matters** |\n",
      "|-----------------|------------------------|--------------------|\n",
      "| **Top Branch** | *Why reasoning matters* ‚Äì LLMs that only ‚Äútalk like humans‚Äù (e.g., GPT‚Äë3.5) lack the ability to *think* step‚Äëby‚Äëstep. | Sets the problem: we need models that *reason*, not just *regurgitate*. |\n",
      "| **Middle Limb** | *Two‚Äëstep evolution* ‚Äì From ‚Äúanswer‚Äëonly‚Äù to **reasoning LLMs** (e.g., OpenAI‚Äôs O‚ÇÇ, DeepMind‚Äôs COG, etc.). | Introduces the timeline (Dec‚ÄØ2022‚ÄØ‚Üí‚ÄØnow) when reasoning became a research priority. |\n",
      "| **Bottom Leaf** | *Goal of the series* ‚Äì Understand *how* we can endow LLMs with reasoning powers. | Guides the rest of the course (methods, experiments, hands‚Äëon). |\n",
      "\n",
      "---\n",
      "\n",
      "## 2Ô∏è‚É£  The Branch of **Inference‚ÄëTime Compute Scaling**  \n",
      "\n",
      "> **Analogy:** A chess grandmaster spends minutes weighing variations before moving; a model that ‚Äúthinks longer‚Äù uses more compute at inference time and often gets a better answer.  \n",
      "\n",
      "### 2.1  Core Idea  \n",
      "- **Definition:** *Test‚Äëtime (inference‚Äëtime) compute* = the amount of computational resources (tokens, forward passes, FLOPs) the model spends **before** emitting its final output.  \n",
      "- **Intuition:** When we tell a model to ‚Äúthink more,‚Äù we increase the depth/width of its internal chain of reasoning, which typically raises accuracy on complex tasks.\n",
      "\n",
      "### 2.2  Example Walk‚ÄëThrough  \n",
      "\n",
      "| Step | Action | Tokens Used | Reasoning |\n",
      "|------|--------|------------|-----------|\n",
      "| 1 | ‚ÄúRoger started with 5 balls.‚Äù | 4 | Sets the initial state. |\n",
      "| 2 | ‚ÄúTwo cans √ó 3 balls each = 6 balls.‚Äù | 11 | Performs multiplication. |\n",
      "| 3 | ‚Äú5 + 6 = 11.‚Äù | 5 | Final addition. |\n",
      "| **Total** | **Answer = 11** | **‚âà20 tokens** (vs. 3 tokens for a direct answer) | More tokens ‚Üí more deliberation ‚Üí same or improved correctness. |\n",
      "\n",
      "### 2.3  Empirical Scaling Plot (PaLM example)  \n",
      "\n",
      "- **X‚Äëaxis:** Model parameter count (62‚ÄØB ‚Üí 540‚ÄØB).  \n",
      "- **Y‚Äëaxis:** Accuracy on GSM‚Äë8K (arithmetic reasoning).  \n",
      "- **Observations:**  \n",
      "  1. **Standard prompting** ‚Üí low accuracy (~5‚Äì10‚ÄØ%).  \n",
      "  2. **Chain‚Äëof‚ÄëThought prompting** ‚Üí steep rise; larger models achieve near‚Äësupervised‚Äëfine‚Äëtune performance *without* any weight updates.  \n",
      "  3. **Zero‚ÄëShot CoT** (just ‚ÄúLet‚Äôs think step‚Äëby‚Äëstep‚Äù) ‚Üí similar gains, especially on 540‚ÄØB models.\n",
      "\n",
      "### 2.4  Why More Compute Helps  \n",
      "\n",
      "| Mechanism | Explanation |\n",
      "|-----------|-------------|\n",
      "| **Deeper reasoning pathways** | Extra forward passes let the model explore intermediate states, akin to ‚Äúmental simulation.‚Äù |\n",
      "| **Error correction** | Mistakes made early can be revised when the model continues reasoning, fixing semantic slips (e.g., confusing ‚Äú4‚ÄØ√ó‚ÄØ6 vs. 48√∑6‚Äù). |\n",
      "| **Emergent ability** | Scaling reveals a *threshold* where the model ‚Äúgets‚Äù the prompting nudge and can follow reasoning chains reliably. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3Ô∏è‚É£  The **Chain‚Äëof‚ÄëThought (CoT)** Branch  \n",
      "\n",
      "### 3.1  What It Is  \n",
      "- **Definition:** Adding a **series of intermediate steps** (natural‚Äëlanguage ‚Äúrationals‚Äù) to the output of a prompt.  \n",
      "- **Implementation:** Provide *few‚Äëshot* examples where **input ‚Üí reasoning steps ‚Üí final answer** are shown.  \n",
      "\n",
      "### 3.2  Structural Anatomy  \n",
      "\n",
      "```\n",
      "Input:  Roger has 5 tennis balls. He buys 2 cans of 3 balls each.\n",
      "Output (CoT):\n",
      "  1. Roger started with 5 balls.          ‚Üê 4 tokens\n",
      "  2. Two cans √ó 3 balls = 6 balls.       ‚Üê 11 tokens\n",
      "  3. 5 + 6 = 11.                         ‚Üê 5 tokens\n",
      "  4. Answer: 11.                         ‚Üê final token\n",
      "```\n",
      "\n",
      "*Key nuance:* The model **generates the reasoning** itself once the pattern is demonstrated.\n",
      "\n",
      "### 3.3  Why It Works  \n",
      "\n",
      "| Insight | Detail |\n",
      "|---------|--------|\n",
      "| **Rationales bridge gaps** | Natural language ‚Äúrationals‚Äù provide a *semantic scaffold* that aligns model activations with logical steps. |\n",
      "| **No architectural change** | Only the prompt changes; the underlying model stays identical. |\n",
      "| **Few‚Äëshot prompting** | 10‚Äë20 demonstration pairs are enough for the model to internalize the reasoning pattern. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4Ô∏è‚É£  **Few‚ÄëShot vs. Zero‚ÄëShot CoT**  \n",
      "\n",
      "| Feature | Few‚ÄëShot CoT | Zero‚ÄëShot CoT |\n",
      "|---------|--------------|---------------|\n",
      "| **Prompt content** | Input‚ÄëOutput pairs **with** explicit reasoning steps. | No examples; only a cue like ‚ÄúLet‚Äôs think step‚Äëby‚Äëstep.‚Äù |\n",
      "| **Cognitive load on model** | Must match the *format* of the examples (structure, token count). | Must *self‚Äëgenerate* a reasoning chain from the cue alone. |\n",
      "| **Typical performance** | Often higher on very large models because they can copy the demonstrated style. | Slightly lower but still improves over vanilla prompting; easier to deploy. |\n",
      "| **Implementation cost** | Requires curating demonstration set. | Minimal overhead ‚Äì just a short instruction. |\n",
      "\n",
      "---\n",
      "\n",
      "## 5Ô∏è‚É£  Comparative Experiments (Hands‚ÄëOn Lens)\n",
      "\n",
      "### 5.1  Project‚ÄØ1 ‚Äì *Model‚ÄëSize √ó CoT Effect*  \n",
      "- **Goal:** Empirically verify that CoT only shines on sufficiently large models.  \n",
      "- **Setup:**  \n",
      "  - Datasets: GSM‚Äë8K (arithmetic), logical/symbolic reasoning sets.  \n",
      "  - Models: FLAN‚ÄëT5‚Äësmall (80‚ÄØM), FLAN‚ÄëT5‚Äëbase, FLAN‚ÄëT5‚Äëlarge, LLaMA‚Äë52‚ÄØB, LLaMA‚Äë7‚ÄØB, TinyLlama‚Äë1.1‚ÄØB.  \n",
      "  - Evaluation function: `is_correct(answer, ground_truth)`.  \n",
      "- **Findings (in lecture):**  \n",
      "  - Accuracy ‚âà 0‚Äì5‚ÄØ% for all models ‚â§ 7‚ÄØB with CoT.  \n",
      "  - Accuracy climbs sharply only beyond ~50‚ÄØB parameters.  \n",
      "  - Matches paper‚Äôs claim: *Chain‚Äëof‚ÄëThought is an emergent ability of model scaling*.  \n",
      "\n",
      "### 5.2  Project‚ÄØ2 ‚Äì *Three Prompting Strategies*  \n",
      "- **Baselines:**  \n",
      "  1. **Vanilla Few‚ÄëShot** (direct answer).  \n",
      "  2. **Few‚ÄëShot CoT** (explicit steps).  \n",
      "  3. **Zero‚ÄëShot CoT** (‚ÄúLet‚Äôs think step‚Äëby‚Äëstep‚Äù).  \n",
      "- **Models Tested:** FLAN‚Äë52‚ÄØB & LLaMA‚Äë7‚ÄØB.  \n",
      "- **Tasks:** Logical puzzles, symbolic flips, simple commonsense questions.  \n",
      "- **Result Summary:**  \n",
      "  - Zero‚ÄëShot CoT outperforms both baselines (‚âà20‚ÄØ% ‚Üí 0‚ÄØ% vanilla).  \n",
      "  - Few‚ÄëShot CoT remains superior when enough demonstrations are provided.  \n",
      "  - Demonstrates that *a single cue* (‚Äúthink step‚Äëby‚Äëstep‚Äù) can unlock reasoning without curated examples.  \n",
      "\n",
      "---\n",
      "\n",
      "## 6Ô∏è‚É£  **Key Takeaways ‚Äì The Owl‚Äôs Nuggets**  \n",
      "\n",
      "| **Takeaway** | **Explanation (Owl‚ÄëStyle)** |\n",
      "|--------------|-----------------------------|\n",
      "| **Reasoning is a scaling phenomenon** | Like a fledgling eventually learning to soar, only the biggest brains can reliably follow multi‚Äëstep reasoning when prompted. |\n",
      "| **Inference‚Äëtime compute ‚â† training cost** | You don‚Äôt need to retrain; you simply spend more ‚Äúthinking time‚Äù (tokens) at inference, which mobilizes latent reasoning pathways. |\n",
      "| **Chain‚Äëof‚ÄëThought is prompt engineering, not architecture** | The model‚Äôs *knowledge* stays the same; the *behaviour* emerges from how you frame the question. |\n",
      "| **Zero‚ÄëShot CoT is a low‚Äëfriction entry point** | Just add ‚ÄúLet‚Äôs think step‚Äëby‚Äëstep‚Äù ‚Äì cheap, universal, works even without demonstration examples. |\n",
      "| **Empirical gains are robust across domains** | Arithmetic, common‚Äësense, and symbolic tasks all benefit, though the magnitude varies (bigger for logical puzzles, modest for simple commonsense). |\n",
      "| **Model size matters, but prompt design matters more** | A modest cue can unlock massive performance jumps if the model is large enough; conversely, even a large model may flounder with a poorly phrased prompt. |\n",
      "| **Practical implication** | For production, you can often avoid fine‚Äëtuning: just switch to CoT/Zero‚ÄëShot CoT prompts and allocate a modest compute budget at inference. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7Ô∏è‚É£  **Suggested Reading Path (Owl‚ÄëGuided Bibliography)**  \n",
      "\n",
      "1. **\"Chain of Thought Prompting Elicits Reasoning in Large Language Models\"** ‚Äì Google Brain, 2022.  \n",
      "2. **\"Solving NLP Tasks with Zero‚ÄëShot CoT\"** ‚Äì Wei et‚ÄØal., 2022 (arXiv).  \n",
      "3. **\"Scaling Laws for Neural Language Models\"** ‚Äì Kaplan et‚ÄØal., 2020 (parameter‚Äëvs‚Äëperformance trends).  \n",
      "4. **\"Emergent Abilities of Large Language Models\"** ‚Äì Wei, Lee, et‚ÄØal., 2022.  \n",
      "5. **Hands‚ÄëOn Colab notebooks** ‚Äì shared during lecture (install `transformers`, `datasets`, `accelerate`; load GSM‚Äë8K; test FLAN‚ÄëT5‚Äësmall ‚Üí LLaMA‚Äë7B).  \n",
      "\n",
      "---\n",
      "\n",
      "## 8Ô∏è‚É£  **Next Flight Steps (Course Continuation)**  \n",
      "\n",
      "- **Module‚ÄØ1 Recap:** Inference‚Äëtime compute scaling ‚Üí deeper dive into *budget‚Äëaware* reasoning (e.g., dynamic token allocation, ‚Äúself‚Äëconsistency‚Äù).  \n",
      "- **Module‚ÄØ2 Preview:** *Model‚Äëbased* reasoning (e.g., Retrieval‚ÄëAugmented Generation, Tool‚ÄëUse, Tool‚Äëformer).  \n",
      "- **Hands‚ÄëOn Labs:**  \n",
      "  1. **Lab‚ÄØA:** Build your own CoT few‚Äëshot prompts for GSM‚Äë8K; evaluate scaling curves.  \n",
      "  2. **Lab‚ÄØB:** Implement Zero‚ÄëShot CoT; compare with vanilla prompting across logical tasks.  \n",
      "  3. **Lab‚ÄØC:** Run a ‚Äúfrom‚Äëscratch‚Äù 7‚ÄØB LLaMA clone; test CoT on commonsense QA.  \n",
      "\n",
      "---\n",
      "\n",
      "### üéØ  Bottom‚ÄëLine (Owl‚ÄëWisdom)\n",
      "\n",
      "> **When we give LLMs *time* to think‚Äîby allocating extra inference compute and nudging them with a simple ‚Äúthink step‚Äëby‚Äëstep‚Äù cue‚Äîwe unlock a latent reasoning engine that scales with model size.**  \n",
      "> \n",
      "> *Use the right prompt, give the model enough ‚Äúflight time,‚Äù and watch it transform from a chatterbox into a problem‚Äësolving predator.*  \n",
      "\n",
      "--- \n",
      "\n",
      "*End of Owl‚ÄëContext notes.*  \n",
      "\n",
      "\n",
      "\n",
      "--- \n",
      "\n",
      "*Prepared for students seeking a deep, structured understanding of reasoning‚Äëbased LLMs, with an emphasis on inference‚Äëtime compute scaling, Chain‚Äëof‚ÄëThought prompting, and practical experimentation.*\n"
     ]
    }
   ],
   "source": [
    "# Print response in a split manner for easier reading\n",
    "# If response is a complex object, print its attributes separately\n",
    "print(\"Content:\\n\", response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7692eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
